{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/z1079621/storage/embeddings\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import imp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "import src.models.bilm\n",
    "import src.models.bilm.data\n",
    "# imp.reload(src.models.bilm.data)\n",
    "# imp.reload(src.models.bilm)\n",
    "\n",
    "from src import DATA_DIR\n",
    "from src.models.bilm.model import dump_bilm_embeddings, dump_bilm_embeddings_with_tokens\n",
    "from src.models.bilm.data import Vocabulary, UnicodeCharsVocabulary\n",
    "\n",
    "print(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_file = os.path.join(DATA_DIR, 'elmo', 'vocab_elmo.txt')\n",
    "elmo_vocab = Vocabulary(vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sentences(x1, x2, batch_size):\n",
    "    for id_sample in range(batch_size):\n",
    "        print(\"Premise:\", elmo_vocab.decode(np.trim_zeros(x1[id_sample])-1))\n",
    "        print(\"Hypo:\", elmo_vocab.decode(np.trim_zeros(x2[id_sample])-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output of models/elmo_test.py\n",
    "elmo_our_input = np.load(os.path.join(DATA_DIR, 'elmo/elmo_our_inp.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_sentences(elmo_our_input[0], elmo_our_input[1], elmo_our_input.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 256, 90)\n"
     ]
    }
   ],
   "source": [
    "print(elmo_our_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_premises = []\n",
    "tokenized_hypothesis = []\n",
    "for id_sample in range(elmo_our_input.shape[1]):\n",
    "    tokenized_premises.append(elmo_vocab.decode(np.trim_zeros(elmo_our_input[0][id_sample])-1).split()[1:-1])\n",
    "    tokenized_hypothesis.append(elmo_vocab.decode(np.trim_zeros(elmo_our_input[1][id_sample])-1).split()[1:-1])\n",
    "    \n",
    "print(\"PREM\")\n",
    "print(tokenized_premises)\n",
    "\n",
    "# print(\"HYPO\")\n",
    "# print(tokenized_hypothesis)\n",
    "\n",
    "length_premises = [len(sent) for sent in tokenized_premises]\n",
    "length_hypothesis = [len(sent) for sent in tokenized_hypothesis]\n",
    "\n",
    "# print(\"Length premises:\", length_premises)\n",
    "# print(\"Length hypothesis:\", length_hypothesis)\n",
    "\n",
    "print(\"Total premises\", len(length_premises))\n",
    "print(\"Total hypothesis\", len(length_hypothesis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sess = tf.InteractiveSession()\n",
    "# elmo = hub.Module(\"/home/z1079621/storage/embeddings/elmohub/\", trainable=False)\n",
    "# sess.run(tf.global_variables_initializer())\n",
    "# sess.run(tf.tables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def send_to_hub(tokens_without_padding):\n",
    "#     maxlen = 0\n",
    "#     for sentence in tokens_without_padding:\n",
    "#         maxlen = max(maxlen, len(sentence))\n",
    "        \n",
    "#     tokens = []\n",
    "#     lengths = []\n",
    "    \n",
    "#     for sentence in tokens_without_padding:\n",
    "#         sentence = sentence[:maxlen]\n",
    "#         lengths.append(len(sentence))\n",
    "#         sentence = sentence + [\"\" for _ in range(max(0, maxlen - len(sentence)))]\n",
    "#         tokens.append(sentence)\n",
    "\n",
    "#     return elmo(\n",
    "#         inputs={\n",
    "#             \"tokens\": tokens,\n",
    "#             \"sequence_len\": lengths\n",
    "#         },\n",
    "#         signature=\"tokens\",\n",
    "#         as_dict=True\n",
    "#     )['word_emb'].eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hub_embeddings_premises = send_to_hub(tokenized_premises)\n",
    "# hub_embeddings_hypothesis = send_to_hub(tokenized_hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get embeddings from tf-bilm model\n",
    "\n",
    "dataset_file = os.path.join(DATA_DIR, 'elmo', 'dataset_file.txt')\n",
    "with open(dataset_file, 'w') as fout:\n",
    "    for sentence in tokenized_premises:\n",
    "        fout.write(' '.join(sentence) + '\\n')\n",
    "    for sentence in tokenized_hypothesis:\n",
    "        fout.write(' '.join(sentence) + '\\n')\n",
    "\n",
    "datadir = os.path.join(DATA_DIR, 'elmo')\n",
    "options_file = os.path.join(datadir, 'options.json')\n",
    "weight_file = os.path.join(datadir, 'lm_weights.hdf5')\n",
    "\n",
    "# Dump the embeddings to a file. Run this once for your dataset.\n",
    "token_embedding_file = os.path.join(datadir, 'elmo_token_embeddings.hdf5')\n",
    "outut_embedding_file = os.path.join(datadir, 'elmo_bilm_sample_embeddings.hdf5')\n",
    "with tf.variable_scope('', reuse=tf.AUTO_REUSE):\n",
    "    dump_bilm_embeddings_with_tokens(\n",
    "        vocab_file, dataset_file, options_file, weight_file, \n",
    "        token_embedding_file, \n",
    "        outut_embedding_file\n",
    "    )\n",
    "\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading saved embeddings from file\n",
    "\n",
    "hub_embeddings_premises = []    \n",
    "hub_embeddings_hypothesis = []\n",
    "\n",
    "with h5py.File(outut_embedding_file, 'r') as fin:\n",
    "    num_sentences = len(list(fin.keys()))\n",
    "    for i in range(num_sentences // 2):\n",
    "        cur = fin['%d' % i][...]\n",
    "        hub_embeddings_premises.append(cur)                              \n",
    "    for i in range(num_sentences // 2, num_sentences):\n",
    "        cur = fin['%d' % i][...]\n",
    "        hub_embeddings_hypothesis.append(cur)\n",
    "\n",
    "# maxlen_premises_shapes = np.array([0]*3)\n",
    "# maxlen_hypothesis_shapes = np.array([0]*3)\n",
    "\n",
    "# # Padding\n",
    "\n",
    "# for premise, hypo in zip(hub_embeddings_premises, hub_embeddings_hypothesis):\n",
    "#     maxlen_premises_shapes = np.maximum(maxlen_premises_shapes, premise.shape)\n",
    "#     maxlen_hypothesis_shapes = np.maximum(maxlen_hypothesis_shapes, hypo.shape)\n",
    "    \n",
    "# for i in range(len(hub_embeddings_premises)):\n",
    "#     hub_embeddings_premises[i] = np.pad(\n",
    "#         hub_embeddings_premises[i], \n",
    "#         list(zip([0]*3, maxlen_premises_shapes - hub_embeddings_premises[i].shape)),\n",
    "#         mode='constant', constant_values=0)\n",
    "    \n",
    "# for i in range(len(hub_embeddings_hypothesis)):\n",
    "#     hub_embeddings_hypothesis[i] = np.pad(\n",
    "#         hub_embeddings_hypothesis[i], \n",
    "#         list(zip([0]*3, maxlen_hypothesis_shapes - hub_embeddings_hypothesis[i].shape)),\n",
    "#         mode='constant', constant_values=0)\n",
    "    \n",
    "# hub_embeddings_premises = np.array(hub_embeddings_premises)\n",
    "# hub_embeddings_hypothesis = np.array(hub_embeddings_hypothesis)\n",
    "\n",
    "# for e in hub_embeddings_premises:\n",
    "#     print(e.shape)\n",
    "    \n",
    "# for e in hub_embeddings_hypothesis:\n",
    "#     print(e.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one_sample_dataset_file = os.path.join(DATA_DIR, 'elmo', 'one_sample_dataset_file.txt')\n",
    "# with open(one_sample_dataset_file, 'w') as fout:\n",
    "#     fout.write(' '.join(['A', 'dog', 'trying', 'to', 'catch', 'a', 'ball', '.']) + '\\n')\n",
    "\n",
    "# with tf.variable_scope('', reuse=tf.AUTO_REUSE):\n",
    "#     dump_bilm_embeddings_with_tokens(\n",
    "#         vocab_file, one_sample_dataset_file, options_file, weight_file, \n",
    "#         token_embedding_file, \n",
    "#         outut_embedding_file\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.allclose(one_sample_embedding, hub_embeddings_premises[1]))\n",
    "\n",
    "# plot_diff(one_sample_embedding, hub_embeddings_premises[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one_sample_embedding = None\n",
    "# with h5py.File(outut_embedding_file, 'r') as fin:\n",
    "#     one_sample_embedding = fin['0'][...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1537996159.1067402, '/home/z1079621/storage/embeddings/elmo/elmo_our_out_5.npy'), (1537996070.995028, '/home/z1079621/storage/embeddings/elmo/elmo_our_out_4.npy'), (1537974072.5013933, '/home/z1079621/storage/embeddings/elmo/elmo_our_out_3.npy'), (1537973627.0866625, '/home/z1079621/storage/embeddings/elmo/elmo_our_out_2.npy')]\n",
      "/home/z1079621/storage/embeddings/elmo/elmo_our_out_5.npy\n",
      "(2, 256, 3, 90, 1024)\n",
      "Matches 256/256: \n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "paths_and_times = []\n",
    "dirpath = os.path.join(DATA_DIR, 'elmo/elmo_our_out*')\n",
    "for name in glob.glob(dirpath):\n",
    "    path = Path(name)\n",
    "    paths_and_times.append((path.stat().st_mtime, name))\n",
    "\n",
    "paths_and_times = sorted(paths_and_times, key=lambda x: -x[0])\n",
    "_, paths_sorted = zip(*paths_and_times)\n",
    "\n",
    "print(paths_and_times)\n",
    "\n",
    "elmo_our_output_path = paths_sorted[0]\n",
    "# elmo_our_output_prev_path = paths_sorted[1]\n",
    "\n",
    "print(elmo_our_output_path)\n",
    "# print(elmo_our_output_prev_path)\n",
    "\n",
    "elmo_our_output = np.load(elmo_our_output_path)\n",
    "# elmo_our_output_prev = np.load(elmo_our_output_prev_path)\n",
    "\n",
    "print(elmo_our_output.shape)\n",
    "\n",
    "our_embeddings_premises = elmo_our_output[0]\n",
    "our_embeddings_hypothesis = elmo_our_output[1]\n",
    "\n",
    "# our_embeddings_prev_premises = elmo_our_output_prev[0]\n",
    "# our_embeddings_prev_hypothesis = elmo_our_output_prev[1]\n",
    "\n",
    "matches = 0\n",
    "total_samples = 0\n",
    "\n",
    "# print(len(tokenized_premises))\n",
    "# print(our_embeddings_premises.shape)\n",
    "# print(len(hub_embeddings_premises))\n",
    "\n",
    "def plot_diff(our_embeddings, hub_embeddings):\n",
    "    for i, (our_layer, hub_layer) in enumerate(zip(our_embeddings, hub_embeddings)):\n",
    "        diff = (our_layer[:,:512] - hub_layer[:,:512]).flatten()\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.hist(diff, bins=50)\n",
    "        ax.set_title('diff fwd ')\n",
    "\n",
    "        diff = (our_layer[0,:512] - hub_layer[0,:512]).flatten()\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.hist(diff, bins=50)\n",
    "        ax.set_title('diff fwd 0')\n",
    "\n",
    "        diff = (our_layer[:,:-512] - hub_layer[:,:-512]).flatten()\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.hist(diff, bins=50)\n",
    "        ax.set_title('diff bck')\n",
    "\n",
    "for sample_id, (premise, our_embeddings, hub_embeddings) in enumerate(zip(\n",
    "        tokenized_premises, our_embeddings_premises, hub_embeddings_premises)):\n",
    "    \n",
    "    our_embeddings = our_embeddings[:, :hub_embeddings.shape[1], :]\n",
    "    assert len(premise) + 2 == hub_embeddings.shape[1]\n",
    "    \n",
    "    layers_match = []\n",
    "    \n",
    "    for i, (our_layer, hub_layer) in enumerate(zip(our_embeddings, hub_embeddings)):\n",
    "        allclose = [np.allclose(our_word, hub_word, atol=1e-2) for our_word, hub_word in zip(our_layer, hub_layer)]\n",
    "        layers_match.append(np.all(allclose))\n",
    "    \n",
    "    matches += np.all(layers_match)\n",
    "    total_samples += 1\n",
    "    \n",
    "    if not np.all(layers_match):\n",
    "        plot_diff(our_embeddings, hub_embeddings)\n",
    "        break\n",
    "        \n",
    "print(\"Matches %d/%d: \" % (matches, total_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Split concatenated input embeddings\n",
    "# def split_embeddings(embeddings): # [batch, layer, word, dim]\n",
    "#     after_split = []\n",
    "#     for id_sen in range(embeddings.shape[0]):\n",
    "#         input_embeddings = embeddings[id_sen][0] # [batch][layer]\n",
    "#         assert np.allclose(input_embeddings[:, :512], input_embeddings[:, -512:])\n",
    "#         input_embeddings = input_embeddings[:, :512]\n",
    "#         after_split.append(input_embeddings)\n",
    "#     return np.array(after_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our_embeddings_premises = split_embeddings(elmo_our_output[0])\n",
    "# our_embeddings_hypothesis = split_embeddings(elmo_our_output[1])\n",
    "# print(our_embeddings_premises.shape)\n",
    "# print(our_embeddings_hypothesis.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_mean_var(word_emb, title):\n",
    "#     for id_sen in range(word_emb.shape[0]):\n",
    "#         mean = np.mean(word_emb[id_sen], axis=-1)\n",
    "#         var = np.var(word_emb[id_sen], axis=-1)\n",
    "#         fig, ax = plt.subplots()\n",
    "#         ax.set_title(\"%s mean\" % title)\n",
    "#         ax.bar(x=range(mean.shape[0]), height=mean)\n",
    "        \n",
    "#         fig, ax = plt.subplots()\n",
    "#         ax.set_title(\"%s var\" % title)\n",
    "#         ax.bar(x=range(var.shape[0]), height=var)\n",
    "        \n",
    "# #         print(mean)\n",
    "# #         print(var)\n",
    "#         print(\"non-zeros: \", np.trim_zeros(np.sum(np.abs(word_emb[id_sen]), axis=-1)).shape[0])\n",
    "        \n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Czy nasze embeddingi dla paddingu są zero?\n",
    "# get_mean_var(our_embeddings_premises, \"premises\")\n",
    "# get_mean_var(our_embeddings_hypothesis, \"hypo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For padding we have zero embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our_embeddings_premises = our_embeddings_premises[:, :hub_embeddings_premises.shape[1] ] # for <S>, </S> tokens\n",
    "# our_embeddings_hypothesis = our_embeddings_hypothesis[:, :hub_embeddings_hypothesis.shape[1] ] # for <S>, </S> tokens\n",
    "# print(our_embeddings_premises.shape)\n",
    "# print(our_embeddings_hypothesis.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for our_embedding, hub_embedding in zip(our_embeddings_premises, hub_embeddings_premises):\n",
    "#     print(our_embedding.shape, hub_embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # <S>, </S> tokens match?\n",
    "# def s_tokens_match(premises, length_premises, hypothesis, length_hypothesis):\n",
    "#     print(np.allclose(premises[0][0], premises[1][0]))\n",
    "#     print(np.allclose(premises[0][length_premises[0]], premises[1][length_premises[1]]))\n",
    "#     print(np.allclose(hypothesis[0][0], hypothesis[1][0]))\n",
    "#     print(np.allclose(hypothesis[0][length_hypothesis[0]], hypothesis[1][length_hypothesis[1]]))\n",
    "#     print(np.allclose(premises[0][0], hypothesis[1][0]))\n",
    "    \n",
    "# s_tokens_match(our_embeddings_premises, \n",
    "#                length_premises,\n",
    "#                our_embeddings_hypothesis,\n",
    "#                length_hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Czy embeddingi zgadzają się po usunięciu <S> i </S>?\n",
    "\n",
    "# # in: [batch, word, dim]\n",
    "# def compare_norms_and_dots(our, hub):\n",
    "#     max_batch_size = 1\n",
    "#     for id_sen in range(min(max_batch_size, our.shape[0])):\n",
    "#         our_norms = np.linalg.norm(our[id_sen], axis=1, keepdims=True) + np.finfo(np.float32).eps\n",
    "#         hub_norms = np.linalg.norm(hub[id_sen], axis=1, keepdims=True) + np.finfo(np.float32).eps\n",
    "#         dots = np.sum(np.multiply(our[id_sen] / our_norms, hub[id_sen] / hub_norms), axis=1)\n",
    "#         print(\"Our norms:\", np.squeeze(our_norms))\n",
    "#         print(\"Hub norms:\", np.squeeze(hub_norms))\n",
    "#         print(\"Diff:\", np.squeeze(np.abs(our_norms - hub_norms)))\n",
    "#         print(\"Dots:\", np.squeeze(dots))\n",
    "\n",
    "# compare_norms_and_dots(our_embeddings_premises[:, 1:-1], hub_embeddings_premises)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import PCA\n",
    "\n",
    "# pca = PCA(n_components=2)\n",
    "# # word_emb_all = np.concatenate([word_emb_our, word_emb_their], axis=0)\n",
    "\n",
    "# word_emb_all = np.concatenate([word_emb_their], axis=0)\n",
    "# word_emb_all = np.reshape(word_emb_all, [-1, 512])\n",
    "\n",
    "# word_emb_all_pca = pca.fit_transform(word_emb_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_emb_all_pca = np.reshape(word_emb_all_pca, [-1, 21, 2])\n",
    "# word_emb_all_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_emb_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_prem_hypo(word_emb_all_pca):\n",
    "#     labels = ['p1', 'h1', 'p2', 'h2']\n",
    "#     colors = ['red', 'orange', 'black', 'gray']\n",
    "\n",
    "#     # print(word_emb_all_pca.shape)\n",
    "\n",
    "#     fig, ax = plt.subplots()\n",
    "#     for i in range(word_emb_all_pca.shape[0]):\n",
    "#         label = labels[i]\n",
    "#         x, y = zip(*word_emb_all_pca[i])\n",
    "#         ax.scatter(x=x, y=y, c=colors[i], label=label)\n",
    "\n",
    "#     ax.legend()\n",
    "#     plt.show()\n",
    "    \n",
    "# plot_prem_hypo(word_emb_all_pca)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
