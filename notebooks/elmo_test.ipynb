{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/z1079621/storage/embeddings\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import h5py\n",
    "import imp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "import src.models.bilm\n",
    "import src.models.bilm.data\n",
    "# imp.reload(src.models.bilm.data)\n",
    "# imp.reload(src.models.bilm)\n",
    "\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from src import DATA_DIR\n",
    "from src.models.bilm.model import dump_bilm_embeddings, dump_bilm_embeddings_with_tokens\n",
    "from src.models.bilm.data import Vocabulary, UnicodeCharsVocabulary\n",
    "\n",
    "print(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_file = os.path.join(DATA_DIR, 'elmo', 'vocab_elmo.txt')\n",
    "elmo_vocab = Vocabulary(vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_version_number():\n",
    "    prefix_to_look_for = 'elmo_our_embeddings_'\n",
    "    paths_and_times = []\n",
    "    dirpath = os.path.join(DATA_DIR, 'elmo/%s*' % prefix_to_look_for)\n",
    "    for name in glob.glob(dirpath):\n",
    "        path = Path(name)\n",
    "        paths_and_times.append((path.stat().st_mtime, name))\n",
    "\n",
    "    paths_and_times = sorted(paths_and_times, key=lambda x: -x[0])\n",
    "    if paths_and_times:\n",
    "        _, paths_sorted = zip(*paths_and_times)\n",
    "        latest_path = paths_sorted[0]\n",
    "        latest_number = int(re.findall('%s(\\d+)' % prefix_to_look_for, latest_path)[0])\n",
    "    else:\n",
    "        latest_number = -1\n",
    "    return latest_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sentences(x1, x2, batch_size):\n",
    "    for id_sample in range(batch_size):\n",
    "        print(\"Premise:\", elmo_vocab.decode(np.trim_zeros(x1[id_sample])-1))\n",
    "        print(\"Hypo:\", elmo_vocab.decode(np.trim_zeros(x2[id_sample])-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latest_version_number = get_latest_version_number()\n",
    "latest_version_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_path = os.path.join(\n",
    "    DATA_DIR, 'elmo/elmo_our_inputs_%d.npy' % (latest_version_number))\n",
    "embeddings_path = os.path.join(\n",
    "    DATA_DIR, 'elmo/elmo_our_embeddings_%d.npy' % (latest_version_number))\n",
    "weighted_embeddings_path = os.path.join(\n",
    "    DATA_DIR, 'elmo/elmo_our_weighted_embeddings_%d.npy' % (latest_version_number))\n",
    "\n",
    "elmo_our_inputs = np.load(inputs_path)\n",
    "elmo_our_embeddings = np.load(embeddings_path)\n",
    "elmo_our_weighted_embeddings = np.load(weighted_embeddings_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 38)\n",
      "(64, 3, 38, 1024)\n",
      "(64, 38, 1024)\n"
     ]
    }
   ],
   "source": [
    "print(elmo_our_inputs.shape)\n",
    "print(elmo_our_embeddings.shape)\n",
    "print(elmo_our_weighted_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Golfer',\n",
      "  'takes',\n",
      "  'a',\n",
      "  'swing',\n",
      "  'at',\n",
      "  'the',\n",
      "  'ball',\n",
      "  'as',\n",
      "  'spectators',\n",
      "  'watch',\n",
      "  '.'],\n",
      " ['A', 'family', 'of', '6', 'on', 'a', 'beach', '.'],\n",
      " ['A',\n",
      "  'young',\n",
      "  'lady',\n",
      "  'in',\n",
      "  'motion',\n",
      "  'on',\n",
      "  'cobblestone',\n",
      "  'in',\n",
      "  'front',\n",
      "  'of',\n",
      "  'multicolored',\n",
      "  'houses',\n",
      "  '.'],\n",
      " ['A',\n",
      "  'group',\n",
      "  'of',\n",
      "  'people',\n",
      "  'listening',\n",
      "  'to',\n",
      "  'a',\n",
      "  'man',\n",
      "  'speak',\n",
      "  'from',\n",
      "  'a',\n",
      "  'booth',\n",
      "  'labeled',\n",
      "  '``',\n",
      "  'Teva',\n",
      "  'Neuroscience',\n",
      "  \"''\",\n",
      "  '.'],\n",
      " ['Child', 'playing', 'with', 'soccer', 'ball', 'in', 'a', 'field', '.'],\n",
      " ['A',\n",
      "  'young',\n",
      "  'girl',\n",
      "  'in',\n",
      "  'sunglasses',\n",
      "  ',',\n",
      "  'poses',\n",
      "  'for',\n",
      "  'the',\n",
      "  'camera',\n",
      "  '.'],\n",
      " ['The',\n",
      "  'man',\n",
      "  'in',\n",
      "  'the',\n",
      "  'white',\n",
      "  'shirt',\n",
      "  'tugging',\n",
      "  'at',\n",
      "  'a',\n",
      "  'long',\n",
      "  'rope',\n",
      "  '.'],\n",
      " ['A',\n",
      "  'child',\n",
      "  'smiling',\n",
      "  'while',\n",
      "  'holding',\n",
      "  'a',\n",
      "  'knob',\n",
      "  'and',\n",
      "  'pushing',\n",
      "  'a',\n",
      "  'button',\n",
      "  'on',\n",
      "  'a',\n",
      "  'control',\n",
      "  'panel',\n",
      "  '.'],\n",
      " ['A',\n",
      "  'young',\n",
      "  'girl',\n",
      "  'is',\n",
      "  'kicking',\n",
      "  'a',\n",
      "  'soccer',\n",
      "  'ball',\n",
      "  'in',\n",
      "  'the',\n",
      "  'grass',\n",
      "  '.'],\n",
      " ['A',\n",
      "  'young',\n",
      "  'person',\n",
      "  'in',\n",
      "  'black',\n",
      "  'pants',\n",
      "  'and',\n",
      "  'a',\n",
      "  'brown',\n",
      "  'fedora',\n",
      "  'uses',\n",
      "  'their',\n",
      "  'mobile',\n",
      "  'device',\n",
      "  'on',\n",
      "  'a',\n",
      "  'city',\n",
      "  'sidewalk',\n",
      "  '.'],\n",
      " ['A',\n",
      "  'woman',\n",
      "  'in',\n",
      "  'a',\n",
      "  'white',\n",
      "  'dress',\n",
      "  'is',\n",
      "  'standing',\n",
      "  'in',\n",
      "  'front',\n",
      "  'of',\n",
      "  'a',\n",
      "  'parade',\n",
      "  'of',\n",
      "  'uniformed',\n",
      "  'men',\n",
      "  'marching',\n",
      "  'down',\n",
      "  'the',\n",
      "  'street',\n",
      "  '.'],\n",
      " ['A',\n",
      "  'little',\n",
      "  'girl',\n",
      "  'is',\n",
      "  'creeping',\n",
      "  'behind',\n",
      "  'two',\n",
      "  'ducks',\n",
      "  'at',\n",
      "  'the',\n",
      "  'top',\n",
      "  'of',\n",
      "  'a',\n",
      "  'cliff',\n",
      "  'overlooking',\n",
      "  'the',\n",
      "  'ocean',\n",
      "  '.'],\n",
      " ['A',\n",
      "  'baby',\n",
      "  'girl',\n",
      "  'is',\n",
      "  'attempting',\n",
      "  'to',\n",
      "  'climb',\n",
      "  'out',\n",
      "  'of',\n",
      "  'her',\n",
      "  'green',\n",
      "  'crib'],\n",
      " ['Here',\n",
      "  'is',\n",
      "  'a',\n",
      "  'young',\n",
      "  'man',\n",
      "  'sitting',\n",
      "  'on',\n",
      "  'the',\n",
      "  'ground',\n",
      "  'looking',\n",
      "  'for',\n",
      "  'inspiration',\n",
      "  'for',\n",
      "  'a',\n",
      "  'new',\n",
      "  'drawing',\n",
      "  'that',\n",
      "  'is',\n",
      "  'about',\n",
      "  'architecture',\n",
      "  '.'],\n",
      " ['A',\n",
      "  'man',\n",
      "  'in',\n",
      "  'a',\n",
      "  'white',\n",
      "  'button',\n",
      "  'up',\n",
      "  'shirt',\n",
      "  'smiling',\n",
      "  'and',\n",
      "  'laughing',\n",
      "  'at',\n",
      "  'the',\n",
      "  'camera',\n",
      "  '.'],\n",
      " ['A',\n",
      "  'woman',\n",
      "  'in',\n",
      "  'a',\n",
      "  'denim',\n",
      "  'jumper',\n",
      "  'and',\n",
      "  'another',\n",
      "  'woman',\n",
      "  'in',\n",
      "  'a',\n",
      "  'navy',\n",
      "  'blue',\n",
      "  'jacket',\n",
      "  'are',\n",
      "  'overlooking',\n",
      "  'a',\n",
      "  'mountain',\n",
      "  'range',\n",
      "  '.'],\n",
      " ['Five',\n",
      "  'adults',\n",
      "  'are',\n",
      "  'playing',\n",
      "  'cards',\n",
      "  'while',\n",
      "  'sitting',\n",
      "  'outside',\n",
      "  'at',\n",
      "  'a',\n",
      "  'picnic',\n",
      "  'table',\n",
      "  'in',\n",
      "  'their',\n",
      "  'backyard',\n",
      "  '.'],\n",
      " ['A', 'boy', 'is', 'inside', 'of', 'a', 'vehicle', '.'],\n",
      " ['A',\n",
      "  'girl',\n",
      "  'in',\n",
      "  'a',\n",
      "  'sweater',\n",
      "  'is',\n",
      "  'at',\n",
      "  'a',\n",
      "  'temple',\n",
      "  'with',\n",
      "  'a',\n",
      "  'variety',\n",
      "  'of',\n",
      "  'plants',\n",
      "  '.'],\n",
      " ['Two',\n",
      "  'men',\n",
      "  'in',\n",
      "  'brown',\n",
      "  'jackets',\n",
      "  'walking',\n",
      "  'away',\n",
      "  'from',\n",
      "  'a',\n",
      "  'parking',\n",
      "  'garage',\n",
      "  '.'],\n",
      " ['Elderly',\n",
      "  'gentleman',\n",
      "  'wearing',\n",
      "  'a',\n",
      "  'beige',\n",
      "  'sweater',\n",
      "  ',',\n",
      "  'using',\n",
      "  'a',\n",
      "  'hammer',\n",
      "  'and',\n",
      "  'anvil',\n",
      "  'to',\n",
      "  'pound',\n",
      "  'on',\n",
      "  'heated',\n",
      "  'in',\n",
      "  'a',\n",
      "  'blacksmith',\n",
      "  'shop',\n",
      "  '.'],\n",
      " ['A',\n",
      "  'guard',\n",
      "  'stands',\n",
      "  'watch',\n",
      "  'at',\n",
      "  'a',\n",
      "  'monument',\n",
      "  'as',\n",
      "  'tourists',\n",
      "  'look',\n",
      "  'on',\n",
      "  '.'],\n",
      " ['Three', 'firemen', 'are', 'conversing', 'next', 'to', 'a', 'firetruck', '.'],\n",
      " ['women',\n",
      "  'dressed',\n",
      "  'up',\n",
      "  'as',\n",
      "  'zombie',\n",
      "  'wearing',\n",
      "  'white',\n",
      "  'and',\n",
      "  'pink',\n",
      "  'top'],\n",
      " ['A',\n",
      "  'nun',\n",
      "  'dressed',\n",
      "  'in',\n",
      "  'a',\n",
      "  'white',\n",
      "  ',',\n",
      "  'blue',\n",
      "  ',',\n",
      "  'and',\n",
      "  'black',\n",
      "  'robe',\n",
      "  'handing',\n",
      "  'an',\n",
      "  'older',\n",
      "  'man',\n",
      "  'something',\n",
      "  '.'],\n",
      " ['A', 'little', 'girl', 'is', 'playing', 'on', 'the', 'swings', '.'],\n",
      " ['Two', 'dogs', 'running', 'over', 'packed', 'dirt', 'and', 'rocks'],\n",
      " ['A',\n",
      "  'main',\n",
      "  'street',\n",
      "  'scene',\n",
      "  'of',\n",
      "  'a',\n",
      "  'small',\n",
      "  'town',\n",
      "  'with',\n",
      "  'an',\n",
      "  'overhead',\n",
      "  'welcome',\n",
      "  'sign',\n",
      "  'that',\n",
      "  'says',\n",
      "  '``',\n",
      "  'Welcome',\n",
      "  'to',\n",
      "  'Golden',\n",
      "  \"''\",\n",
      "  '.'],\n",
      " ['A',\n",
      "  'bmx',\n",
      "  'rider',\n",
      "  'does',\n",
      "  'a',\n",
      "  'trick',\n",
      "  'against',\n",
      "  'a',\n",
      "  'graffiti',\n",
      "  'covered',\n",
      "  'wall',\n",
      "  '.'],\n",
      " ['Three',\n",
      "  'young',\n",
      "  'roller',\n",
      "  'hockey',\n",
      "  'players',\n",
      "  'making',\n",
      "  'a',\n",
      "  'shot',\n",
      "  'to',\n",
      "  'goal',\n",
      "  '.'],\n",
      " ['A', 'young', 'woman', 'sitting', 'on', 'a', 'park', 'bench', '.'],\n",
      " ['Mother', 'and', 'daughter', 'walking', 'through', 'a', 'city', '.'],\n",
      " ['Many',\n",
      "  'people',\n",
      "  'wearing',\n",
      "  'headphones',\n",
      "  'are',\n",
      "  'sitting',\n",
      "  'in',\n",
      "  'blue',\n",
      "  'chairs',\n",
      "  'looking',\n",
      "  'at',\n",
      "  'computers',\n",
      "  '.'],\n",
      " ['Three', 'guys', 'are', 'wearing', 'red', 'shirts', '.'],\n",
      " ['Two',\n",
      "  'people',\n",
      "  'standing',\n",
      "  'at',\n",
      "  'the',\n",
      "  'edge',\n",
      "  'of',\n",
      "  'a',\n",
      "  'snow',\n",
      "  'covered',\n",
      "  'cliff',\n",
      "  '.'],\n",
      " ['Three', 'people', 'participate', 'in', 'rock', 'climbing', '.'],\n",
      " ['A',\n",
      "  'man',\n",
      "  'with',\n",
      "  'a',\n",
      "  'mohawk',\n",
      "  'and',\n",
      "  'makeup',\n",
      "  'is',\n",
      "  'talking',\n",
      "  'to',\n",
      "  'a',\n",
      "  'girl',\n",
      "  'dressed',\n",
      "  'in',\n",
      "  'a',\n",
      "  'purple',\n",
      "  'hat',\n",
      "  '.'],\n",
      " ['Two',\n",
      "  'women',\n",
      "  'are',\n",
      "  'laughing',\n",
      "  'on',\n",
      "  'the',\n",
      "  'floor',\n",
      "  'with',\n",
      "  'toy',\n",
      "  'wands',\n",
      "  '.'],\n",
      " ['soccer', 'players', 'in', 'red', 'jerseys', 'practice', '.'],\n",
      " ['A',\n",
      "  'bearded',\n",
      "  'man',\n",
      "  'wearing',\n",
      "  'glasses',\n",
      "  ',',\n",
      "  'a',\n",
      "  'red',\n",
      "  'shirt',\n",
      "  ',',\n",
      "  'and',\n",
      "  'a',\n",
      "  'hat',\n",
      "  'is',\n",
      "  'reading',\n",
      "  'a',\n",
      "  'map',\n",
      "  'while',\n",
      "  'sitting',\n",
      "  'in',\n",
      "  'a',\n",
      "  'car',\n",
      "  '.'],\n",
      " ['A', 'hand', 'is', 'holding', 'flippers', '.'],\n",
      " ['Football', 'players', 'in', 'the', 'middle', 'of', 'a', 'play', '.'],\n",
      " ['A', 'bewildered', 'child', 'examines', 'a', 'peculiar', 'plant', '.'],\n",
      " ['Kids', 'playing', 'softball', '.'],\n",
      " ['Two',\n",
      "  'dogs',\n",
      "  'are',\n",
      "  'on',\n",
      "  'a',\n",
      "  'white',\n",
      "  'carpet',\n",
      "  'with',\n",
      "  'one',\n",
      "  'of',\n",
      "  'the',\n",
      "  'dogs',\n",
      "  'showing',\n",
      "  'its',\n",
      "  'fangs',\n",
      "  '.'],\n",
      " ['Two',\n",
      "  'people',\n",
      "  'wearing',\n",
      "  'red',\n",
      "  'are',\n",
      "  'sitting',\n",
      "  'on',\n",
      "  'a',\n",
      "  'brick',\n",
      "  'wall',\n",
      "  'drinking',\n",
      "  'coffee',\n",
      "  '.'],\n",
      " ['Four',\n",
      "  'men',\n",
      "  'on',\n",
      "  'a',\n",
      "  'street',\n",
      "  'where',\n",
      "  'the',\n",
      "  'paint',\n",
      "  'is',\n",
      "  'peeling',\n",
      "  'from',\n",
      "  'the',\n",
      "  'walls',\n",
      "  'of',\n",
      "  'the',\n",
      "  'buildings',\n",
      "  '.'],\n",
      " ['A', 'cook', 'prepares', 'food', 'in', 'a', 'restaurant', 'kitchen', '.'],\n",
      " ['A',\n",
      "  'woman',\n",
      "  'holding',\n",
      "  'a',\n",
      "  'black',\n",
      "  'and',\n",
      "  'white',\n",
      "  'hand',\n",
      "  'bag',\n",
      "  'is',\n",
      "  'looking',\n",
      "  'at',\n",
      "  'the',\n",
      "  'bus',\n",
      "  'schedule',\n",
      "  '.'],\n",
      " ['Two',\n",
      "  'recreational',\n",
      "  'soccer',\n",
      "  'teams',\n",
      "  'stand',\n",
      "  'at',\n",
      "  'attention',\n",
      "  'looking',\n",
      "  'at',\n",
      "  'each',\n",
      "  'other',\n",
      "  'in',\n",
      "  'clean',\n",
      "  'uniforms',\n",
      "  'before',\n",
      "  'playing',\n",
      "  'soccer',\n",
      "  '.'],\n",
      " ['Three',\n",
      "  'women',\n",
      "  'sit',\n",
      "  'around',\n",
      "  'a',\n",
      "  'raised',\n",
      "  'fire',\n",
      "  'pit',\n",
      "  'cooking',\n",
      "  'skewered',\n",
      "  'young',\n",
      "  'pigs',\n",
      "  '.'],\n",
      " ['A',\n",
      "  'young',\n",
      "  'blond-haired',\n",
      "  'boy',\n",
      "  'in',\n",
      "  'a',\n",
      "  'white',\n",
      "  'shirt',\n",
      "  'and',\n",
      "  'red',\n",
      "  'pants',\n",
      "  'running',\n",
      "  'threw',\n",
      "  'a',\n",
      "  'park',\n",
      "  '.'],\n",
      " ['Kids',\n",
      "  'play',\n",
      "  'soccer',\n",
      "  'in',\n",
      "  'front',\n",
      "  'of',\n",
      "  'a',\n",
      "  'marble',\n",
      "  'building',\n",
      "  'with',\n",
      "  'giant',\n",
      "  'oak',\n",
      "  'doors',\n",
      "  '.'],\n",
      " ['a',\n",
      "  'man',\n",
      "  'working',\n",
      "  'with',\n",
      "  'plants',\n",
      "  'trying',\n",
      "  'to',\n",
      "  'get',\n",
      "  'his',\n",
      "  'setup',\n",
      "  'perfect',\n",
      "  '.'],\n",
      " ['Pedestrians',\n",
      "  'strolling',\n",
      "  'along',\n",
      "  'a',\n",
      "  'brick',\n",
      "  'walkway',\n",
      "  'tween',\n",
      "  'high',\n",
      "  'buildings',\n",
      "  '.'],\n",
      " ['People',\n",
      "  'are',\n",
      "  'walking',\n",
      "  'through',\n",
      "  'a',\n",
      "  'market',\n",
      "  ',',\n",
      "  'browsing',\n",
      "  'over',\n",
      "  'merchandise',\n",
      "  'and',\n",
      "  'making',\n",
      "  'purchases',\n",
      "  '.'],\n",
      " ['Two',\n",
      "  'little',\n",
      "  'girls',\n",
      "  'are',\n",
      "  'playing',\n",
      "  'in',\n",
      "  'their',\n",
      "  'house',\n",
      "  ',',\n",
      "  'one',\n",
      "  'is',\n",
      "  'on',\n",
      "  'the',\n",
      "  'floor',\n",
      "  'grinning',\n",
      "  'evilly',\n",
      "  'wearing',\n",
      "  'pink',\n",
      "  'pajamas',\n",
      "  'and',\n",
      "  'the',\n",
      "  'younger',\n",
      "  'one',\n",
      "  'is',\n",
      "  'standing',\n",
      "  'over',\n",
      "  'her',\n",
      "  'wearing',\n",
      "  'a',\n",
      "  'hairnet',\n",
      "  'and',\n",
      "  'mask',\n",
      "  'and',\n",
      "  'purple',\n",
      "  'pajamas',\n",
      "  '.'],\n",
      " ['A', 'mariachi', 'band', 'plays', 'a', 'song', 'on', 'the', 'sidewalk', '.'],\n",
      " ['Two', 'Chinese', 'women', 'are', 'inside', 'a', 'shop', '.'],\n",
      " ['Two',\n",
      "  'men',\n",
      "  'and',\n",
      "  'one',\n",
      "  'woman',\n",
      "  'wearing',\n",
      "  'suits',\n",
      "  'walking',\n",
      "  'down',\n",
      "  'a',\n",
      "  'street',\n",
      "  '.'],\n",
      " ['Girls', 'jumping', 'in', 'air', 'after', 'finishing', 'race', '.'],\n",
      " ['Six', 'people', 'standing', 'on', 'a', 'sidewalk', 'laughing', '.'],\n",
      " ['A', 'tradition', 'Indian', 'women', 'dancing', '.'],\n",
      " ['A',\n",
      "  'man',\n",
      "  'in',\n",
      "  'a',\n",
      "  'blue',\n",
      "  'shirt',\n",
      "  'is',\n",
      "  'walking',\n",
      "  'and',\n",
      "  'there',\n",
      "  'is',\n",
      "  'a',\n",
      "  'blue',\n",
      "  'bucket',\n",
      "  'that',\n",
      "  'can',\n",
      "  'be',\n",
      "  'seen',\n",
      "  '.']]\n",
      "Number of sentences 64\n"
     ]
    }
   ],
   "source": [
    "tokenized_sentences = []\n",
    "for sentence in elmo_our_inputs:\n",
    "    tokenized_sentences.append(elmo_vocab.decode(np.trim_zeros(sentence)-1).split()[1:-1])\n",
    "    \n",
    "pprint(tokenized_sentences)\n",
    "\n",
    "length_sentences = [len(sent) for sent in tokenized_sentences]\n",
    "print(\"Number of sentences\", len(length_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sess = tf.InteractiveSession()\n",
    "# elmo = hub.Module(\"/home/z1079621/storage/embeddings/elmohub/\", trainable=False)\n",
    "# sess.run(tf.global_variables_initializer())\n",
    "# sess.run(tf.tables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def send_to_hub(tokens_without_padding):\n",
    "#     maxlen = 0\n",
    "#     for sentence in tokens_without_padding:\n",
    "#         maxlen = max(maxlen, len(sentence))\n",
    "        \n",
    "#     tokens = []\n",
    "#     lengths = []\n",
    "    \n",
    "#     for sentence in tokens_without_padding:\n",
    "#         sentence = sentence[:maxlen]\n",
    "#         lengths.append(len(sentence))\n",
    "#         sentence = sentence + [\"\" for _ in range(max(0, maxlen - len(sentence)))]\n",
    "#         tokens.append(sentence)\n",
    "\n",
    "#     return elmo(\n",
    "#         inputs={\n",
    "#             \"tokens\": tokens,\n",
    "#             \"sequence_len\": lengths\n",
    "#         },\n",
    "#         signature=\"tokens\",\n",
    "#         as_dict=True\n",
    "#     )['word_emb'].eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hub_embeddings_premises = send_to_hub(tokenized_premises)\n",
    "# hub_embeddings_hypothesis = send_to_hub(tokenized_hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING SKIP CONNECTIONS\n",
      "[b'Golfer', b'takes', b'a', b'swing', b'at', b'the', b'ball', b'as', b'spectators', b'watch', b'.'] 11\n",
      "[b'A', b'family', b'of', b'6', b'on', b'a', b'beach', b'.'] 8\n",
      "[b'A', b'young', b'lady', b'in', b'motion', b'on', b'cobblestone', b'in', b'front', b'of', b'multicolored', b'houses', b'.'] 13\n",
      "[b'A', b'group', b'of', b'people', b'listening', b'to', b'a', b'man', b'speak', b'from', b'a', b'booth', b'labeled', b'``', b'Teva', b'Neuroscience', b\"''\", b'.'] 18\n",
      "[b'Child', b'playing', b'with', b'soccer', b'ball', b'in', b'a', b'field', b'.'] 9\n",
      "[b'A', b'young', b'girl', b'in', b'sunglasses', b',', b'poses', b'for', b'the', b'camera', b'.'] 11\n",
      "[b'The', b'man', b'in', b'the', b'white', b'shirt', b'tugging', b'at', b'a', b'long', b'rope', b'.'] 12\n",
      "[b'A', b'child', b'smiling', b'while', b'holding', b'a', b'knob', b'and', b'pushing', b'a', b'button', b'on', b'a', b'control', b'panel', b'.'] 16\n",
      "[b'A', b'young', b'girl', b'is', b'kicking', b'a', b'soccer', b'ball', b'in', b'the', b'grass', b'.'] 12\n",
      "[b'A', b'young', b'person', b'in', b'black', b'pants', b'and', b'a', b'brown', b'fedora', b'uses', b'their', b'mobile', b'device', b'on', b'a', b'city', b'sidewalk', b'.'] 19\n",
      "[b'A', b'woman', b'in', b'a', b'white', b'dress', b'is', b'standing', b'in', b'front', b'of', b'a', b'parade', b'of', b'uniformed', b'men', b'marching', b'down', b'the', b'street', b'.'] 21\n",
      "[b'A', b'little', b'girl', b'is', b'creeping', b'behind', b'two', b'ducks', b'at', b'the', b'top', b'of', b'a', b'cliff', b'overlooking', b'the', b'ocean', b'.'] 18\n",
      "[b'A', b'baby', b'girl', b'is', b'attempting', b'to', b'climb', b'out', b'of', b'her', b'green', b'crib'] 12\n",
      "[b'Here', b'is', b'a', b'young', b'man', b'sitting', b'on', b'the', b'ground', b'looking', b'for', b'inspiration', b'for', b'a', b'new', b'drawing', b'that', b'is', b'about', b'architecture', b'.'] 21\n",
      "[b'A', b'man', b'in', b'a', b'white', b'button', b'up', b'shirt', b'smiling', b'and', b'laughing', b'at', b'the', b'camera', b'.'] 15\n",
      "[b'A', b'woman', b'in', b'a', b'denim', b'jumper', b'and', b'another', b'woman', b'in', b'a', b'navy', b'blue', b'jacket', b'are', b'overlooking', b'a', b'mountain', b'range', b'.'] 20\n",
      "[b'Five', b'adults', b'are', b'playing', b'cards', b'while', b'sitting', b'outside', b'at', b'a', b'picnic', b'table', b'in', b'their', b'backyard', b'.'] 16\n",
      "[b'A', b'boy', b'is', b'inside', b'of', b'a', b'vehicle', b'.'] 8\n",
      "[b'A', b'girl', b'in', b'a', b'sweater', b'is', b'at', b'a', b'temple', b'with', b'a', b'variety', b'of', b'plants', b'.'] 15\n",
      "[b'Two', b'men', b'in', b'brown', b'jackets', b'walking', b'away', b'from', b'a', b'parking', b'garage', b'.'] 12\n",
      "[b'Elderly', b'gentleman', b'wearing', b'a', b'beige', b'sweater', b',', b'using', b'a', b'hammer', b'and', b'anvil', b'to', b'pound', b'on', b'heated', b'in', b'a', b'blacksmith', b'shop', b'.'] 21\n",
      "[b'A', b'guard', b'stands', b'watch', b'at', b'a', b'monument', b'as', b'tourists', b'look', b'on', b'.'] 12\n",
      "[b'Three', b'firemen', b'are', b'conversing', b'next', b'to', b'a', b'firetruck', b'.'] 9\n",
      "[b'women', b'dressed', b'up', b'as', b'zombie', b'wearing', b'white', b'and', b'pink', b'top'] 10\n",
      "[b'A', b'nun', b'dressed', b'in', b'a', b'white', b',', b'blue', b',', b'and', b'black', b'robe', b'handing', b'an', b'older', b'man', b'something', b'.'] 18\n",
      "[b'A', b'little', b'girl', b'is', b'playing', b'on', b'the', b'swings', b'.'] 9\n",
      "[b'Two', b'dogs', b'running', b'over', b'packed', b'dirt', b'and', b'rocks'] 8\n",
      "[b'A', b'main', b'street', b'scene', b'of', b'a', b'small', b'town', b'with', b'an', b'overhead', b'welcome', b'sign', b'that', b'says', b'``', b'Welcome', b'to', b'Golden', b\"''\", b'.'] 21\n",
      "[b'A', b'bmx', b'rider', b'does', b'a', b'trick', b'against', b'a', b'graffiti', b'covered', b'wall', b'.'] 12\n",
      "[b'Three', b'young', b'roller', b'hockey', b'players', b'making', b'a', b'shot', b'to', b'goal', b'.'] 11\n",
      "[b'A', b'young', b'woman', b'sitting', b'on', b'a', b'park', b'bench', b'.'] 9\n",
      "[b'Mother', b'and', b'daughter', b'walking', b'through', b'a', b'city', b'.'] 8\n",
      "[b'Many', b'people', b'wearing', b'headphones', b'are', b'sitting', b'in', b'blue', b'chairs', b'looking', b'at', b'computers', b'.'] 13\n",
      "[b'Three', b'guys', b'are', b'wearing', b'red', b'shirts', b'.'] 7\n",
      "[b'Two', b'people', b'standing', b'at', b'the', b'edge', b'of', b'a', b'snow', b'covered', b'cliff', b'.'] 12\n",
      "[b'Three', b'people', b'participate', b'in', b'rock', b'climbing', b'.'] 7\n",
      "[b'A', b'man', b'with', b'a', b'mohawk', b'and', b'makeup', b'is', b'talking', b'to', b'a', b'girl', b'dressed', b'in', b'a', b'purple', b'hat', b'.'] 18\n",
      "[b'Two', b'women', b'are', b'laughing', b'on', b'the', b'floor', b'with', b'toy', b'wands', b'.'] 11\n",
      "[b'soccer', b'players', b'in', b'red', b'jerseys', b'practice', b'.'] 7\n",
      "[b'A', b'bearded', b'man', b'wearing', b'glasses', b',', b'a', b'red', b'shirt', b',', b'and', b'a', b'hat', b'is', b'reading', b'a', b'map', b'while', b'sitting', b'in', b'a', b'car', b'.'] 23\n",
      "[b'A', b'hand', b'is', b'holding', b'flippers', b'.'] 6\n",
      "[b'Football', b'players', b'in', b'the', b'middle', b'of', b'a', b'play', b'.'] 9\n",
      "[b'A', b'bewildered', b'child', b'examines', b'a', b'peculiar', b'plant', b'.'] 8\n",
      "[b'Kids', b'playing', b'softball', b'.'] 4\n",
      "[b'Two', b'dogs', b'are', b'on', b'a', b'white', b'carpet', b'with', b'one', b'of', b'the', b'dogs', b'showing', b'its', b'fangs', b'.'] 16\n",
      "[b'Two', b'people', b'wearing', b'red', b'are', b'sitting', b'on', b'a', b'brick', b'wall', b'drinking', b'coffee', b'.'] 13\n",
      "[b'Four', b'men', b'on', b'a', b'street', b'where', b'the', b'paint', b'is', b'peeling', b'from', b'the', b'walls', b'of', b'the', b'buildings', b'.'] 17\n",
      "[b'A', b'cook', b'prepares', b'food', b'in', b'a', b'restaurant', b'kitchen', b'.'] 9\n",
      "[b'A', b'woman', b'holding', b'a', b'black', b'and', b'white', b'hand', b'bag', b'is', b'looking', b'at', b'the', b'bus', b'schedule', b'.'] 16\n",
      "[b'Two', b'recreational', b'soccer', b'teams', b'stand', b'at', b'attention', b'looking', b'at', b'each', b'other', b'in', b'clean', b'uniforms', b'before', b'playing', b'soccer', b'.'] 18\n",
      "[b'Three', b'women', b'sit', b'around', b'a', b'raised', b'fire', b'pit', b'cooking', b'skewered', b'young', b'pigs', b'.'] 13\n",
      "[b'A', b'young', b'blond-haired', b'boy', b'in', b'a', b'white', b'shirt', b'and', b'red', b'pants', b'running', b'threw', b'a', b'park', b'.'] 16\n",
      "[b'Kids', b'play', b'soccer', b'in', b'front', b'of', b'a', b'marble', b'building', b'with', b'giant', b'oak', b'doors', b'.'] 14\n",
      "[b'a', b'man', b'working', b'with', b'plants', b'trying', b'to', b'get', b'his', b'setup', b'perfect', b'.'] 12\n",
      "[b'Pedestrians', b'strolling', b'along', b'a', b'brick', b'walkway', b'tween', b'high', b'buildings', b'.'] 10\n",
      "[b'People', b'are', b'walking', b'through', b'a', b'market', b',', b'browsing', b'over', b'merchandise', b'and', b'making', b'purchases', b'.'] 14\n",
      "[b'Two', b'little', b'girls', b'are', b'playing', b'in', b'their', b'house', b',', b'one', b'is', b'on', b'the', b'floor', b'grinning', b'evilly', b'wearing', b'pink', b'pajamas', b'and', b'the', b'younger', b'one', b'is', b'standing', b'over', b'her', b'wearing', b'a', b'hairnet', b'and', b'mask', b'and', b'purple', b'pajamas', b'.'] 36\n",
      "[b'A', b'mariachi', b'band', b'plays', b'a', b'song', b'on', b'the', b'sidewalk', b'.'] 10\n",
      "[b'Two', b'Chinese', b'women', b'are', b'inside', b'a', b'shop', b'.'] 8\n",
      "[b'Two', b'men', b'and', b'one', b'woman', b'wearing', b'suits', b'walking', b'down', b'a', b'street', b'.'] 12\n",
      "[b'Girls', b'jumping', b'in', b'air', b'after', b'finishing', b'race', b'.'] 8\n",
      "[b'Six', b'people', b'standing', b'on', b'a', b'sidewalk', b'laughing', b'.'] 8\n",
      "[b'A', b'tradition', b'Indian', b'women', b'dancing', b'.'] 6\n",
      "[b'A', b'man', b'in', b'a', b'blue', b'shirt', b'is', b'walking', b'and', b'there', b'is', b'a', b'blue', b'bucket', b'that', b'can', b'be', b'seen', b'.'] 19\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "# Get embeddings from tf-bilm model\n",
    "\n",
    "dataset_file = os.path.join(DATA_DIR, 'elmo', 'dataset_file.txt')\n",
    "with open(dataset_file, 'w') as fout:\n",
    "    for sentence in tokenized_sentences:\n",
    "        fout.write(' '.join(sentence) + '\\n')\n",
    "\n",
    "datadir = os.path.join(DATA_DIR, 'elmo')\n",
    "options_file = os.path.join(datadir, 'options.json')\n",
    "weight_file = os.path.join(datadir, 'lm_weights.hdf5')\n",
    "\n",
    "# Dump the embeddings to a file. Run this once for your dataset.\n",
    "token_embedding_file = os.path.join(datadir, 'elmo_token_embeddings.hdf5')\n",
    "outut_embedding_file = os.path.join(datadir, 'elmo_bilm_sample_embeddings.hdf5')\n",
    "with tf.variable_scope('', reuse=tf.AUTO_REUSE):\n",
    "    dump_bilm_embeddings_with_tokens(\n",
    "        vocab_file, dataset_file, options_file, weight_file, \n",
    "        token_embedding_file, \n",
    "        outut_embedding_file\n",
    "    )\n",
    "\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading saved embeddings from file\n",
    "\n",
    "hub_embeddings = []    \n",
    "\n",
    "with h5py.File(outut_embedding_file, 'r') as fin:\n",
    "    num_sentences = len(list(fin.keys()))\n",
    "    for i in range(num_sentences):\n",
    "        cur = fin['%d' % i][...]\n",
    "        hub_embeddings.append(cur)                              \n",
    "\n",
    "# maxlen_premises_shapes = np.array([0]*3)\n",
    "# maxlen_hypothesis_shapes = np.array([0]*3)\n",
    "\n",
    "# # Padding\n",
    "\n",
    "# for premise, hypo in zip(hub_embeddings_premises, hub_embeddings_hypothesis):\n",
    "#     maxlen_premises_shapes = np.maximum(maxlen_premises_shapes, premise.shape)\n",
    "#     maxlen_hypothesis_shapes = np.maximum(maxlen_hypothesis_shapes, hypo.shape)\n",
    "    \n",
    "# for i in range(len(hub_embeddings_premises)):\n",
    "#     hub_embeddings_premises[i] = np.pad(\n",
    "#         hub_embeddings_premises[i], \n",
    "#         list(zip([0]*3, maxlen_premises_shapes - hub_embeddings_premises[i].shape)),\n",
    "#         mode='constant', constant_values=0)\n",
    "    \n",
    "# for i in range(len(hub_embeddings_hypothesis)):\n",
    "#     hub_embeddings_hypothesis[i] = np.pad(\n",
    "#         hub_embeddings_hypothesis[i], \n",
    "#         list(zip([0]*3, maxlen_hypothesis_shapes - hub_embeddings_hypothesis[i].shape)),\n",
    "#         mode='constant', constant_values=0)\n",
    "    \n",
    "# hub_embeddings_premises = np.array(hub_embeddings_premises)\n",
    "# hub_embeddings_hypothesis = np.array(hub_embeddings_hypothesis)\n",
    "\n",
    "# for e in hub_embeddings_premises:\n",
    "#     print(e.shape)\n",
    "    \n",
    "# for e in hub_embeddings_hypothesis:\n",
    "#     print(e.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one_sample_dataset_file = os.path.join(DATA_DIR, 'elmo', 'one_sample_dataset_file.txt')\n",
    "# with open(one_sample_dataset_file, 'w') as fout:\n",
    "#     fout.write(' '.join(['A', 'dog', 'trying', 'to', 'catch', 'a', 'ball', '.']) + '\\n')\n",
    "\n",
    "# with tf.variable_scope('', reuse=tf.AUTO_REUSE):\n",
    "#     dump_bilm_embeddings_with_tokens(\n",
    "#         vocab_file, one_sample_dataset_file, options_file, weight_file, \n",
    "#         token_embedding_file, \n",
    "#         outut_embedding_file\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.allclose(one_sample_embedding, hub_embeddings_premises[1]))\n",
    "\n",
    "# plot_diff(one_sample_embedding, hub_embeddings_premises[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one_sample_embedding = None\n",
    "# with h5py.File(outut_embedding_file, 'r') as fin:\n",
    "#     one_sample_embedding = fin['0'][...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_diff(our_embeddings, hub_embeddings):\n",
    "    for i, (our_layer, hub_layer) in enumerate(zip(our_embeddings, hub_embeddings)):\n",
    "        diff = (our_layer[:,:512] - hub_layer[:,:512]).flatten()\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.hist(diff, bins=50)\n",
    "        ax.set_title('diff fwd ')\n",
    "\n",
    "        diff = (our_layer[0,:512] - hub_layer[0,:512]).flatten()\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.hist(diff, bins=50)\n",
    "        ax.set_title('diff fwd 0')\n",
    "\n",
    "        diff = (our_layer[:,:-512] - hub_layer[:,:-512]).flatten()\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.hist(diff, bins=50)\n",
    "        ax.set_title('diff bck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 3, 38, 1024)\n",
      "Matches 64/64: \n"
     ]
    }
   ],
   "source": [
    "our_embeddings = np.load(embeddings_path)\n",
    "print(our_embeddings.shape)\n",
    "\n",
    "matches = 0\n",
    "total_samples = 0\n",
    "\n",
    "for sample_id, (sentence, our_embedding, hub_embedding) in enumerate(zip(\n",
    "        tokenized_sentences, our_embeddings, hub_embeddings)):\n",
    "    \n",
    "    our_embedding = our_embedding[:, :hub_embedding.shape[1], :]\n",
    "    assert len(sentence) + 2 == hub_embedding.shape[1]\n",
    "    \n",
    "    layers_match = []\n",
    "    \n",
    "    for i, (our_layer, hub_layer) in enumerate(zip(our_embedding, hub_embedding)):\n",
    "        are_words_close = []\n",
    "        for our_word, hub_word in zip(our_layer, hub_layer):\n",
    "            are_words_close.append(np.allclose(our_word, hub_word, atol=1e-2))\n",
    "        layers_match.append(np.all(are_words_close))\n",
    "    \n",
    "    matches += np.all(layers_match)\n",
    "    total_samples += 1\n",
    "    \n",
    "    if not np.all(layers_match):\n",
    "        plot_diff(our_embedding, hub_embedding)\n",
    "        break\n",
    "        \n",
    "print(\"Matches %d/%d: \" % (matches, total_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Split concatenated input embeddings\n",
    "# def split_embeddings(embeddings): # [batch, layer, word, dim]\n",
    "#     after_split = []\n",
    "#     for id_sen in range(embeddings.shape[0]):\n",
    "#         input_embeddings = embeddings[id_sen][0] # [batch][layer]\n",
    "#         assert np.allclose(input_embeddings[:, :512], input_embeddings[:, -512:])\n",
    "#         input_embeddings = input_embeddings[:, :512]\n",
    "#         after_split.append(input_embeddings)\n",
    "#     return np.array(after_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our_embeddings_premises = split_embeddings(elmo_our_output[0])\n",
    "# our_embeddings_hypothesis = split_embeddings(elmo_our_output[1])\n",
    "# print(our_embeddings_premises.shape)\n",
    "# print(our_embeddings_hypothesis.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_mean_var(word_emb, title):\n",
    "#     for id_sen in range(word_emb.shape[0]):\n",
    "#         mean = np.mean(word_emb[id_sen], axis=-1)\n",
    "#         var = np.var(word_emb[id_sen], axis=-1)\n",
    "#         fig, ax = plt.subplots()\n",
    "#         ax.set_title(\"%s mean\" % title)\n",
    "#         ax.bar(x=range(mean.shape[0]), height=mean)\n",
    "        \n",
    "#         fig, ax = plt.subplots()\n",
    "#         ax.set_title(\"%s var\" % title)\n",
    "#         ax.bar(x=range(var.shape[0]), height=var)\n",
    "        \n",
    "# #         print(mean)\n",
    "# #         print(var)\n",
    "#         print(\"non-zeros: \", np.trim_zeros(np.sum(np.abs(word_emb[id_sen]), axis=-1)).shape[0])\n",
    "        \n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Czy nasze embeddingi dla paddingu są zero?\n",
    "# get_mean_var(our_embeddings_premises, \"premises\")\n",
    "# get_mean_var(our_embeddings_hypothesis, \"hypo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For padding we have zero embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our_embeddings_premises = our_embeddings_premises[:, :hub_embeddings_premises.shape[1] ] # for <S>, </S> tokens\n",
    "# our_embeddings_hypothesis = our_embeddings_hypothesis[:, :hub_embeddings_hypothesis.shape[1] ] # for <S>, </S> tokens\n",
    "# print(our_embeddings_premises.shape)\n",
    "# print(our_embeddings_hypothesis.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for our_embedding, hub_embedding in zip(our_embeddings_premises, hub_embeddings_premises):\n",
    "#     print(our_embedding.shape, hub_embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # <S>, </S> tokens match?\n",
    "# def s_tokens_match(premises, length_premises, hypothesis, length_hypothesis):\n",
    "#     print(np.allclose(premises[0][0], premises[1][0]))\n",
    "#     print(np.allclose(premises[0][length_premises[0]], premises[1][length_premises[1]]))\n",
    "#     print(np.allclose(hypothesis[0][0], hypothesis[1][0]))\n",
    "#     print(np.allclose(hypothesis[0][length_hypothesis[0]], hypothesis[1][length_hypothesis[1]]))\n",
    "#     print(np.allclose(premises[0][0], hypothesis[1][0]))\n",
    "    \n",
    "# s_tokens_match(our_embeddings_premises, \n",
    "#                length_premises,\n",
    "#                our_embeddings_hypothesis,\n",
    "#                length_hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Czy embeddingi zgadzają się po usunięciu <S> i </S>?\n",
    "\n",
    "# # in: [batch, word, dim]\n",
    "# def compare_norms_and_dots(our, hub):\n",
    "#     max_batch_size = 1\n",
    "#     for id_sen in range(min(max_batch_size, our.shape[0])):\n",
    "#         our_norms = np.linalg.norm(our[id_sen], axis=1, keepdims=True) + np.finfo(np.float32).eps\n",
    "#         hub_norms = np.linalg.norm(hub[id_sen], axis=1, keepdims=True) + np.finfo(np.float32).eps\n",
    "#         dots = np.sum(np.multiply(our[id_sen] / our_norms, hub[id_sen] / hub_norms), axis=1)\n",
    "#         print(\"Our norms:\", np.squeeze(our_norms))\n",
    "#         print(\"Hub norms:\", np.squeeze(hub_norms))\n",
    "#         print(\"Diff:\", np.squeeze(np.abs(our_norms - hub_norms)))\n",
    "#         print(\"Dots:\", np.squeeze(dots))\n",
    "\n",
    "# compare_norms_and_dots(our_embeddings_premises[:, 1:-1], hub_embeddings_premises)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import PCA\n",
    "\n",
    "# pca = PCA(n_components=2)\n",
    "# # word_emb_all = np.concatenate([word_emb_our, word_emb_their], axis=0)\n",
    "\n",
    "# word_emb_all = np.concatenate([word_emb_their], axis=0)\n",
    "# word_emb_all = np.reshape(word_emb_all, [-1, 512])\n",
    "\n",
    "# word_emb_all_pca = pca.fit_transform(word_emb_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_emb_all_pca = np.reshape(word_emb_all_pca, [-1, 21, 2])\n",
    "# word_emb_all_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_emb_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_prem_hypo(word_emb_all_pca):\n",
    "#     labels = ['p1', 'h1', 'p2', 'h2']\n",
    "#     colors = ['red', 'orange', 'black', 'gray']\n",
    "\n",
    "#     # print(word_emb_all_pca.shape)\n",
    "\n",
    "#     fig, ax = plt.subplots()\n",
    "#     for i in range(word_emb_all_pca.shape[0]):\n",
    "#         label = labels[i]\n",
    "#         x, y = zip(*word_emb_all_pca[i])\n",
    "#         ax.scatter(x=x, y=y, c=colors[i], label=label)\n",
    "\n",
    "#     ax.legend()\n",
    "#     plt.show()\n",
    "    \n",
    "# plot_prem_hypo(word_emb_all_pca)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
